{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67f67359-c1a8-4443-b9be-7838e0d1cc55",
   "metadata": {},
   "source": [
    "# Прогнозируем временные ряды продаж товаров\n",
    "В этом задании предлагается проанализировать датасет продаж различных товаров и построить предсказание на 24 дня вперед с `2018-12-07`.\n",
    "\n",
    "В ноутбуке содержится ряд вопросов, которые подскажут, на что стоит обратить внимание при прогнозировании. Ответы на них следует оформлять текстом и иллюстрировать кодом/визуализациями.\n",
    "\n",
    "\n",
    "Подробное описание датасетов содержится в файле `dataset/README.md`.\n",
    "\n",
    "<b>При выполнении задания нельзя использовать библиотеку etna.</b>\n",
    "<!-- Кроме самих целевых временных рядов, можно будет использовать исторические данные о проведенных акциях.  \n",
    "[Подробное описание датасета](https://data.world/data-society/causal-effects-in-time-series) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434c01ae",
   "metadata": {},
   "source": [
    "***\n",
    "# 0. Загрузка датасета\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ded3d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "import ipywidgets as widgets\n",
    "\n",
    "javascript_functions = {False: \"hide()\", True: \"show()\"}\n",
    "button_descriptions  = {False: \"Показать код\", True: \"Скрыть код\"}\n",
    "def toggle_code(state):\n",
    "    output_string = \"<script>$(\\\"div.input\\\").{}</script>\"\n",
    "    output_args   = (javascript_functions[state],)\n",
    "    output        = output_string.format(*output_args)\n",
    "    display(HTML(output))\n",
    "def button_action(value):\n",
    "    state = value.new\n",
    "    toggle_code(state)\n",
    "    value.owner.description = button_descriptions[state]\n",
    "state = False\n",
    "toggle_code(state)\n",
    "button = widgets.ToggleButton(state, description = button_descriptions[state])\n",
    "button.observe(button_action, \"value\")\n",
    "display(button)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768ecbcd-8a95-4af6-8353-af9838917e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1669b860-1fa1-458b-8ab0-0a70ac17de2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# загружаем датасет с продажами продуктов\n",
    "df_products = pd.read_csv(\"dataset/products.csv\", index_col=0)\n",
    "\n",
    "# загружаем датасет с рекламными акциями\n",
    "df_promotions = pd.read_csv(\"dataset/promotions.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d914016f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_products.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3ec60c-1aaa-4784-81a4-52700db2cd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Датасет с продажами: {df_products.shape}\")\n",
    "print(f\"Датасет с акциями: {df_promotions.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fa06a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_promotions.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c3df4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9be199",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from IPython.display import HTML\n",
    "#from IPython.display import display\n",
    "# Взято https://stackoverflow.com/questions/31517194/how-to-hide-one-specific-cell-input-or-output-in-ipython-notebook\n",
    "#tag = HTML('''<script>\n",
    "#code_show=true; \n",
    "#function code_toggle() {\n",
    "#    if (code_show){\n",
    "#        $('div.cell.code_cell.rendered.selected div.input').hide();\n",
    "#    } else {\n",
    "#        $('div.cell.code_cell.rendered.selected div.input').show();\n",
    "#    }\n",
    "#    code_show = !code_show\n",
    "#} \n",
    "#$( document ).ready(code_toggle);\n",
    "#</script>\n",
    "#Чтобы показать/скрыть данную ячейку кода нажмите <a href=\"javascript:code_toggle()\">здесь</a>.''')\n",
    "#display(tag)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff19f865",
   "metadata": {},
   "source": [
    "### Посмотрим, как выглядят ряды, которые нужно будет спрогнозировать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d9d565",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "fig = go.Figure()\n",
    "for products in df_products.columns[0:10]:\n",
    "    fig.add_trace(go.Scatter(x=df_products.index, y=df_products[products], name = products))\n",
    "    fig.update_xaxes(\n",
    "        tickformat=\"%b\\n%Y\")\n",
    "fig.update_xaxes(rangeslider_visible=True, \n",
    "                 title_text = \"Дата\", \n",
    "                 title_standoff = 25, \n",
    "                 rangeselector=dict(\n",
    "                     buttons=list([\n",
    "                         dict(count=1, label=\"1м\", step=\"month\", stepmode=\"backward\"),\n",
    "                         dict(count=6, label=\"6м\", step=\"month\", stepmode=\"backward\"),\n",
    "                         dict(count=1, label=\"1г\", step=\"year\", stepmode=\"backward\"),\n",
    "                         dict(label=\"все\", step=\"all\")])))\n",
    "fig.update_yaxes(title_text = \"Продажи\", title_standoff = 25)\n",
    "fig.update_layout(title = \"Набор временных рядов\")\n",
    "fig.show()\n",
    "\n",
    "fig = go.Figure()\n",
    "for products in df_products.columns[20:30]:\n",
    "    fig.add_trace(go.Scatter(x=df_products.index, y=df_products[products], name = products))\n",
    "    fig.update_xaxes(\n",
    "        tickformat=\"%b\\n%Y\")\n",
    "fig.update_xaxes(rangeslider_visible=True, \n",
    "                 title_text = \"Дата\", \n",
    "                 title_standoff = 25, \n",
    "                 rangeselector=dict(\n",
    "                     buttons=list([\n",
    "                         dict(count=1, label=\"1м\", step=\"month\", stepmode=\"backward\"),\n",
    "                         dict(count=6, label=\"6м\", step=\"month\", stepmode=\"backward\"),\n",
    "                         dict(count=1, label=\"1г\", step=\"year\", stepmode=\"backward\"),\n",
    "                         dict(label=\"все\", step=\"all\")])))\n",
    "fig.update_yaxes(title_text = \"Продажи\", title_standoff = 25)\n",
    "fig.update_layout(title = \"Набор временных рядов\")\n",
    "fig.show()\n",
    "\n",
    "fig = go.Figure()\n",
    "for products in df_products.columns[10:19]:\n",
    "    fig.add_trace(go.Scatter(x=df_products.index, y=df_products[products], name = products))\n",
    "    fig.update_xaxes(\n",
    "        tickformat=\"%b\\n%Y\")\n",
    "fig.update_xaxes(rangeslider_visible=True, \n",
    "                 title_text = \"Дата\", \n",
    "                 title_standoff = 25, \n",
    "                 rangeselector=dict(\n",
    "                     buttons=list([\n",
    "                         dict(count=1, label=\"1м\", step=\"month\", stepmode=\"backward\"),\n",
    "                         dict(count=6, label=\"6м\", step=\"month\", stepmode=\"backward\"),\n",
    "                         dict(count=1, label=\"1г\", step=\"year\", stepmode=\"backward\"),\n",
    "                         dict(label=\"все\", step=\"all\")])))\n",
    "fig.update_yaxes(title_text = \"Продажи\", title_standoff = 25)\n",
    "fig.update_layout(title = \"Набор временных рядов\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6744fa5d",
   "metadata": {},
   "source": [
    "### Предварительно понятно, что временные ряды обладают ярко-выраженной сезонностью, вероятно коррелируют друг с другом или автокоррелируют. Кроме того, в рядах отсутствует долговременный тренд."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421f5fbd-1be8-47bd-83fa-177557b140a3",
   "metadata": {},
   "source": [
    "***\n",
    "# 1. EDA\n",
    "В этом блоке предлагается изучить исходные данные и ответить на вопросы про них."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20909620",
   "metadata": {},
   "source": [
    "## 1.1. Сезонности и тренды\n",
    "<ol>\n",
    "    <li>Есть ли в данных явно выраженные тренды?</li>\n",
    "    <li>Есть ли в данных сезонность? Если есть, то какая? Почему это может быть важно?</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db528937",
   "metadata": {},
   "source": [
    "Для проверки временных рядов на стационарность - следоватлеьно отсутствие сезонности, проведём для каждого временного ряда тест Дики-Фулера и визуализируем: \n",
    "\n",
    "\n",
    "* Если p-val < 0.01 - <font color='green'>зелёным</font>\n",
    "\n",
    "\n",
    "* Если p-val < 0.05 - <font color='orange'>жёлтым</font>\n",
    "\n",
    "\n",
    "* Если p-val > 0.05, но ADF < Critical val. - <font color='purple'>фиолетовым</font>\n",
    "\n",
    "\n",
    "* Если p-val > 0.05 - <font color='red'>красным</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5340b4e4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# докажем отсутствие тренда првоеркой на стационарность при помощи теста Дики-Фулера\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns   \n",
    "import math\n",
    "\n",
    "i,j=0,0\n",
    "PLOTS_PER_ROW = 3\n",
    "fig, axs = plt.subplots(math.ceil(len(df_products.columns)/PLOTS_PER_ROW),PLOTS_PER_ROW, figsize=(20, 60))\n",
    "\n",
    "for col in df_products.columns:\n",
    "    result = adfuller(df_products[col])\n",
    "    significance_level = 0.05\n",
    "    adf_stat = result[0]\n",
    "    p_val = result[1]\n",
    "    crit_val_1 = result[4]['1%']\n",
    "    crit_val_5 = result[4]['5%']\n",
    "    crit_val_10 = result[4]['10%']\n",
    "    if (p_val < significance_level) & ((adf_stat < crit_val_1)):\n",
    "        linecolor = 'forestgreen' \n",
    "    elif (p_val < significance_level) & (adf_stat < crit_val_5):\n",
    "        linecolor = 'orange'\n",
    "    elif (p_val < significance_level) & (adf_stat < crit_val_10):\n",
    "        linecolor = 'red'\n",
    "    else:\n",
    "        linecolor = 'purple'\n",
    "    axs[i][j].plot(df_products.index, df_products[col], color=linecolor)\n",
    "    axs[i][j].set_title(f'ADF Statistic {adf_stat:0.3f}, p-value: {p_val:0.3f}\\nCritical Values 1%: {crit_val_1:0.3f}, 5%: {crit_val_5:0.3f}, 10%: {crit_val_10:0.3f}', fontsize=14)\n",
    "    axs[i][j].set_ylabel(ylabel=col, fontsize=14)\n",
    "    j+=1\n",
    "    if j%PLOTS_PER_ROW==0:\n",
    "        i+=1\n",
    "        j=0\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8231f975",
   "metadata": {},
   "source": [
    "**На 5% уровне значимости можно сказать, что все временные ряды стационарны согласно статистике Дики-Фулера. Нет ни одного красного графика. Это значит, что в данных нет выраженного тренда.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3022764",
   "metadata": {},
   "source": [
    "***\n",
    "Проведём также анализ Фурье для выявления сезонностей в данных. \n",
    "Видно, что анализ не выделил каких-то больших сезонностей при таком подходе, обращая внимания только на малочастотные гармоники."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb42f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# для определения сезонности мы можем провести гармонический анализ с преобразованием Фурье\n",
    "import heapq\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns   \n",
    "import math\n",
    "import numpy as np\n",
    "lenth_prod = len(df_products.index)\n",
    "ts_ft=np.abs(np.fft.rfft(df_products['product_10']))\n",
    "ts_freq = np.fft.rfftfreq(lenth_prod)*lenth_prod\n",
    "ts = pd.DataFrame(ts_ft, columns = ['ts_ft'])\n",
    "ts_freq = np.fft.rfftfreq(lenth_prod)\n",
    "# Посмотрим на топ 10 самых выделяющихся гармоник для каждой переменной\n",
    "fig = go.Figure()\n",
    "for col in df_products.columns:\n",
    "    ts_ft=np.abs(np.fft.rfft(df_products[col]))\n",
    "    ts_freq = np.fft.rfftfreq(lenth_prod)*lenth_prod\n",
    "    ts = pd.DataFrame(ts_ft, columns = [col])\n",
    "    fig.add_trace(go.Scatter(x=ts.index, y=ts[col], name = col))\n",
    "fig.update_xaxes(rangeslider_visible=True, \n",
    "                 title_text = \"Длина ряда\", \n",
    "                 title_standoff = 25,)\n",
    "fig.update_yaxes(title_text = \"Частота гармоник\", title_standoff = 25)\n",
    "fig.update_layout(title = \"Набор гармоник временных рядов\")\n",
    "fig.show()\n",
    "\n",
    "# можно заметить, что у рядов выделяются частоты на 3-6 днях, меньше на 12-21 днях и последние всплески на 40-50 дней, \n",
    "#что отражает еженедельную, полумесячную и ежемесячную динамики"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9375efb5",
   "metadata": {},
   "source": [
    "Посмотрим на графики автокорреляций (ACF) для каждого временного ряда в размерности до 600 дней, т.к. очевидно, что больше, чем ежегодной сезонности в данных из 3-4 лет нет. К тому же визуально это видно.\n",
    "\n",
    "\n",
    "Тем не менее стоит обратить внимание на максимизацию второй и третей волны на графиках. Если 3 волна превосходит по модулю значимости вторую волну, значит в данных есть четко-выраженная ежегодная сезонность."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4c1bce",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# кроме того, для определения сезонности помогут графики автокорреляции \n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "i,j=0,0\n",
    "PLOTS_PER_ROW = 3\n",
    "fig, axs = plt.subplots(math.ceil(len(df_products.columns)/PLOTS_PER_ROW),PLOTS_PER_ROW, figsize=(20, 60))\n",
    "\n",
    "for col in df_products.columns:\n",
    "    plot_acf(df_products[col], ax=axs[i][j], lags=600)\n",
    "    axs[i][j].set_ylabel(ylabel=col, fontsize=14)\n",
    "    j+=1\n",
    "    if j%PLOTS_PER_ROW==0:\n",
    "        i+=1\n",
    "        j=0\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# в данном случае мы видим, что в рядах есть значимые автокорреляции на разных уровнях: Например product_26\n",
    "# обладает ежегодной сезонностью, а product_27 полугодовой\n",
    "# на основании этих данных продукты можно распределить на кластеры, для которых можно обучить отдельные модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e17e60",
   "metadata": {},
   "source": [
    "Основываясь на данных автокорреляций мы можем подразделить времянные ряды каждого продукта по категориям:\n",
    "\n",
    "\n",
    "* 1/2 года - максимизация на второй волне (6 продуктов)\n",
    "\n",
    "\n",
    "* 3/4 года - максимизация в 3 квартале (2 продукта)\n",
    "\n",
    "\n",
    "* 1 year - максимизация на 3 волне (31 продукта)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb54cff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# мы могли бы записать некоторые условные классы для классификации лагов\n",
    "lag_classes = [\n",
    "    {\n",
    "        \"name\": \"1/2 year\",\n",
    "        \"min\": 120,\n",
    "        \"max\": 200\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"3/4 year\",\n",
    "        \"min\": 200,\n",
    "        \"max\": 300\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"1 year\",\n",
    "        \"min\": 300,\n",
    "        \"max\": 400\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"1/4 year\",\n",
    "        \"min\": 80,\n",
    "        \"max\": 120\n",
    "    }\n",
    "]\n",
    "# Чтобы далее щаписать функцию, классифицирующую временные ряды в зависимости от того, на каком лаге максимизируется их автокорреляция\n",
    "def classify_columns_by_autocorrelation(df, lag_classes):\n",
    "    autocorrelation = {}\n",
    "    class_dataframes = {class_[\"name\"]: pd.DataFrame() for class_ in lag_classes}\n",
    "    for column in df.columns:\n",
    "        autocorr = acf(df[column], nlags=500) # ограничим количество лагов разумным пределом\n",
    "        max_autocorr = max(autocorr[80:]) # не будем использовать первые 80 лагов, поскольку они не интформативны\n",
    "        max_lag = np.argmax(autocorr[80:])+80 \n",
    "        for lag_class in lag_classes:\n",
    "            if max_lag >= lag_class[\"min\"] and max_lag < lag_class[\"max\"]:\n",
    "                autocorrelation[column] = {\"autocorrelation\": max_autocorr, \"lag\": max_lag, \"class\": lag_class[\"name\"]}\n",
    "                class_dataframes[lag_class[\"name\"]][column] = df[column] # и запишем каждый продукт в новые датафрейм в словаре\n",
    "                break\n",
    "    return class_dataframes\n",
    "\n",
    "\n",
    "\n",
    "df = df_products\n",
    "class_df = classify_columns_by_autocorrelation(df, lag_classes)\n",
    "class_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a3e479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# это бы очень пригодилось, если бы мы вручную строили проноз для отдельных типов рядов\n",
    "# однако мы можем просто передать в рнаш датафрейм значения автокорреляций на каждом лаге для каждой переменной, а затем отдать\n",
    "# эти данные модели, которая сама будет искать закономерности\n",
    "def add_autocorr_columns(df, cols):\n",
    "    for col in cols:\n",
    "        autocorr_col = col + '_autocorr'\n",
    "        df[autocorr_col] = acf(df[col], nlags=df.shape[0]-1)\n",
    "        \n",
    "cols_to_autocorr = df_products.columns\n",
    "add_autocorr_columns(df, cols_to_autocorr)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a6e7af",
   "metadata": {},
   "source": [
    "Эти данные можно использовать для построения более точных моделей, разделяя временные ряды на группы перед моделирование и включая наборы автокорреляций или некоторые собственные ряды относительно друго друга с временным лагом, кратным сезонности.\n",
    "Это может привести к понастоящему превосходному моделированию. \n",
    "Но это трудоёмко, поэтому на данный момент достаточно будет данных одного ряда и промоакций."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbea4e3",
   "metadata": {},
   "source": [
    "## 1.2. Масштаб\n",
    "<ol>\n",
    "    <li> Какой масштаб у рядов? </li>\n",
    "    <li> Какой разброс значений внутри каждого ряда? </li>\n",
    "    <li> Может ли это как-то помешать при прогнозировании? </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb7c0a4",
   "metadata": {},
   "source": [
    "Для просмотра более точных данных о переменных, оценке нормальности распределения и визуализации распределений, посмотрим на интерактивный отчёт, созданный в виде HTML файла. Прикреплён в репозитории."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f64bfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# создадим html отчёт о данных для удобства их просмотра. В отчёте представлены распределения, основные статистики, корреляции\n",
    "!jupyter nbextension enable --py widgetsnbextension\n",
    "from pandas_profiling import ProfileReport\n",
    "report = ProfileReport(df_products, title=\"Обзор данных\", sort=\"ascending\")\n",
    "report.to_file(\"ts_report.html\")\n",
    "report # можно заметить, что многие переменные коррелируют друг с другом, как в положительном направлении, так и в отрицательном\n",
    "# кроме того, не все переменные распределены приближенно по нормальному акону распределения, что говорит о том, что не ко всем из них подойдут стандартные параметрические оценки и нормы стандартизации"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ed22a2",
   "metadata": {},
   "source": [
    "Можно говорить о том, что не все временные ряды обладают нормальным распределением. некоторые ряды смещены влево. Однако, допуская большой размер выборки, мы условно примем распределение переменных близко к нормальному. Хотя, конечно, в реальных продуктах необходимо будет скорее снова разбить продукты на группы и приводить распределение к нормальному средсвами логарифмирования, например. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c5aa1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import sys\n",
    "import warnings\n",
    "from joblib import Parallel, delayed\n",
    "from pathlib import Path\n",
    "\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from statsmodels.tsa.deterministic import (CalendarFourier,\n",
    "                                           CalendarSeasonality,\n",
    "                                           CalendarTimeTrend,\n",
    "                                           DeterministicProcess)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import heapq\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns   \n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f5b785",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# построим периодограммы для каждого продукта, чтобы оценить масштаб сезонностей во временных рядах\n",
    "def plot_periodogram(ts, detrend='linear', ax=None):\n",
    "    from scipy.signal import periodogram\n",
    "    fs = pd.Timedelta(\"1Y\") / pd.Timedelta(\"1D\")\n",
    "    freqencies, spectrum = periodogram(\n",
    "        ts,\n",
    "        fs=fs,\n",
    "        detrend=detrend,\n",
    "        window=\"boxcar\",\n",
    "        scaling='spectrum',\n",
    "    )\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots()\n",
    "    ax.step(freqencies, spectrum, color=\"forestgreen\")\n",
    "    ax.set_xscale(\"log\")\n",
    "    ax.set_xticks([1, 2, 4, 12, 52])\n",
    "    ax.set_xticklabels(\n",
    "        [\n",
    "            \"Годовой\",\n",
    "            \"Полугодовой\",\n",
    "            \"Квартальный\",\n",
    "            \"Месячный\",\n",
    "            \"Недельный\",\n",
    "        ],\n",
    "        rotation=30,\n",
    "    )\n",
    "    ax.ticklabel_format(axis=\"y\", style=\"sci\", scilimits=(0, 0))\n",
    "    ax.set_ylabel(\"Спектральная плотность\")\n",
    "    return ax\n",
    "\n",
    "i,j=0,0\n",
    "PLOTS_PER_ROW = 3\n",
    "fig, axs = plt.subplots(math.ceil(len(df_products.columns)/PLOTS_PER_ROW),PLOTS_PER_ROW, figsize=(20, 60))\n",
    "\n",
    "for col in df_products.columns:\n",
    "    plot_periodogram(df_products[col], ax=axs[i][j])\n",
    "    axs[i][j].set_title(\"Периодограмма \"+col)\n",
    "    j+=1\n",
    "    if j%PLOTS_PER_ROW==0:\n",
    "        i+=1\n",
    "        j=0\n",
    "# как можно заметить, и как ранее показал график автокорреляций, большинство рядов обладают годовой и полугодовой сезонностью \n",
    "# так же есть и месячная сезонность\n",
    "# тем не менее, необходимо сделать прогноз на 24 дня, поэтому группировать ряды по сезоным мы не будем"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89b7b17",
   "metadata": {},
   "source": [
    "Здесь мы, грубо говоря, использовали тот же подход к классификации, что использовали ранее с acf. \n",
    "На основе данных периодограмм можно выделить, что полугодовой цикл больше выделяется у продукта 39, 35, 34, 32, 31, 27, 26, 24. Годовой цикл - в основном у всех остальных. В данном случае мы не выделяли 3/4 года. Можно сказать, что группы полностью идентичны. Таким образом, выясненная закономерность устойчива.\n",
    "Если бы мы подбирали модель временных рядов, мы бы могули учесть значения максимизирующих лагов для каждой группы или каждого временного ряда."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5c764a",
   "metadata": {},
   "source": [
    "## 1.3. Аномалии\n",
    "<ol>\n",
    "    <li>Есть ли в рядах выбросы?</li>\n",
    "    <li>Как выбросы могут повлиять на прогнозирование?</li>\n",
    "    <li>Что с ними можно сделать?</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7416e56",
   "metadata": {},
   "source": [
    "Для визуализации выбросов мы визуализировали z-оценки переменных и выбрали для каждой допустимый коридор. Красные точки вне этого коридора инициируются выбросами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985ed7b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6148a257",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "names = list(df_products)\n",
    "def modify_zscore (df): # для определения выбросов посчитаем модифицированную z-оценку\n",
    "    # которая показывает распределения исходя из медианы, а не среднего\n",
    "    # это нужно, потому что не все наши переменные распределены близко к нормальному закону\n",
    "    median = np.median(df)\n",
    "    deviation_from_median = np.array(df)-median\n",
    "    mad = np.median(np.abs(deviation_from_median))\n",
    "    mod_zscore = (0.6745*deviation_from_median)/(mad)\n",
    "    return mod_zscore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36f3166",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from statsmodels.tsa.deterministic import CalendarFourier,DeterministicProcess\n",
    "from warnings import simplefilter\n",
    "\n",
    "# визуализируем выбросы для каждого ряда\n",
    "def plot_anomaly(score_data, threshold, ax, threshold_2):\n",
    "    score_data = score_data.copy().values\n",
    "    ranks = np.linspace(1, len(score_data), len(score_data))\n",
    "    ranks = pd.DataFrame(ranks)\n",
    "    ranks.index=df_products.index\n",
    "    mask_outliers = (score_data>threshold)\n",
    "    mask_outliers_2 = (score_data<threshold_2)\n",
    "    _, ax = plt.subplots(figsize=(15,6))\n",
    "    ax.plot(ranks[mask_outliers], score_data[mask_outliers], 'o', color='red', label='Аномалии ')\n",
    "    ax.plot(ranks[~mask_outliers], score_data[~mask_outliers], 'o', color='forestgreen')\n",
    "    ax.plot(ranks[mask_outliers_2], score_data[mask_outliers_2], 'o', color='red')\n",
    "    ax.axhline(threshold, color='red', label='Таргет-линии', alpha=0.5)\n",
    "    ax.axhline(threshold_2, color='red', alpha=0.5)\n",
    "    ax.legend(loc='upper right')\n",
    "    plt.show()\n",
    "\n",
    "i,j=0,0\n",
    "for col in df_products.columns:\n",
    "    plot_anomaly(pd.DataFrame(modify_zscore(df_products[col]), index = df_products.index), 3.5, ax=axs[i][j], threshold_2=-2.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c957498",
   "metadata": {},
   "source": [
    "Далее мы преобразовали эти точки либо в ближайшие, либо в средние из 3 квартиля распределения. Исключительно в одном продукте, дабы не удалять точки, мы применили усреднеие по 45-55 персентилям."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f233fb2",
   "metadata": {},
   "source": [
    "Полученные оценки можно видеть на графиках ниже:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f174e251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# обработка выбросов. Заменим их данными из соответствующих квантилей\n",
    "for col in df_products.columns:\n",
    "    score_data = pd.DataFrame(modify_zscore(df_products[col]), index = df_products.index)\n",
    "    df_products['mask'] = score_data\n",
    "    df_products[col] = df_products[col].mask(df_products['mask'] > 3.5, df_products[col].quantile(0.95))\n",
    "    df_products[col] = df_products[col].mask(df_products['mask'] < -2.5, df_products[col].quantile(0.05))\n",
    "\n",
    "score_data = pd.DataFrame(modify_zscore(df_products['product_25']), index = df_products.index)\n",
    "df_products['mask'] = score_data\n",
    "df_products['product_25'] = df_products['product_25'].mask(df_products['mask'] > 3.5, df_products['product_25'].quantile(0.9))\n",
    "df_products['product_25'] = df_products['product_25'].mask(df_products['mask'] < -2.5, df_products['product_25'].quantile(0.05))\n",
    "\n",
    "score_data = pd.DataFrame(modify_zscore(df_products['product_36']), index = df_products.index)\n",
    "df_products['mask'] = score_data\n",
    "df_products['product_36'] = df_products['product_36'].mask(df_products['mask'] > 3.5, df_products['product_36'].quantile(0.90))\n",
    "df_products['product_36'] = df_products['product_36'].mask(df_products['mask'] < -2.5, df_products['product_36'].quantile(0.45))\n",
    "df_products = df_products.drop('mask', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9e126c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# и снова визуализирум, чтобы убидеиться \n",
    "for col in df_products.columns: # что выбросы были преобразованы\n",
    "    plot_anomaly(pd.DataFrame(modify_zscore(df_products[col]), index = df_products.index), 3.5, ax=axs[i][j], threshold_2=-2.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e751dd0",
   "metadata": {},
   "source": [
    "## 1.4. Взаимосвязь рядов\n",
    "<ol>\n",
    "    <li>Коррелируют ли ряды между собой?</li>\n",
    "    <li>Можно ли как-то это использовать? Если да, как? Если нет, почему?</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245c5024",
   "metadata": {},
   "source": [
    "### Как мы выяснили ранее во время EDA, определенные ряды коррелируют между собой. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1743e2",
   "metadata": {},
   "source": [
    "Помимо простой парной корреляции, существует ещё такой тест, как тест рядов на причинность: \n",
    "\n",
    "\n",
    "* Тест Гренджера на причинность\n",
    "\n",
    "\n",
    "* Тест Йохансена на коинтеграцию"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee61711",
   "metadata": {},
   "source": [
    "### Тест Гренджера на причинность\n",
    "Показывает, может ли один ряд быть причиной другого. По нижней оси соответственно показано, какие переменные являются причиной, а по вертикальной - какие последсвием в нижнем треугольнике. И наоборот в верхнем треугольнике."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be439afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# помимо корреляции мы можем оценить, воздействуют ли определенные ряды в прошлом на другие ряды в будущем\n",
    "# в этом нам поможет тест Гренджера на причинность и Йохансена на коллинеарность\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "maxlag=25\n",
    "test = 'ssr_chi2test'\n",
    "def grangers_causation_matrix(data, variables, test='ssr_chi2test', verbose=False):\n",
    "    df = pd.DataFrame(np.zeros((len(variables), len(variables))), columns=variables, index=variables)\n",
    "    for c in df.columns:\n",
    "        for r in df.index:\n",
    "            test_result = grangercausalitytests(data[[r, c]], maxlag=maxlag, verbose=False)\n",
    "            p_values = [round(test_result[i+1][0][test][1],4) for i in range(maxlag)]\n",
    "            if verbose: print(f'Y = {r}, X = {c}, P Values = {p_values}')\n",
    "            min_p_value = np.min(p_values)\n",
    "            df.loc[r, c] = min_p_value\n",
    "    df.columns = [var + '_x' for var in variables]\n",
    "    df.index = [var + '_y' for var in variables]\n",
    "    return df\n",
    "# функция записывает p-уровень значимости теста Гренджера на причинность. Иными словами ряды с малыми значениями (до 0,05)\n",
    "# могут быть причинами других по принципе столбец_x причина строки_y.\n",
    "import plotly.express as px\n",
    "\n",
    "fig = px.imshow(grangers_causation_matrix(df_products.iloc[:, 1:30], variables = df_products.iloc[:, 1:30].columns), aspect=\"auto\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb080bde",
   "metadata": {},
   "source": [
    "### Тест Йохансана на коинтеграцию \n",
    "Тест показывает, какие переменные могут быть коинтегрированы с другими на определённом уровне значимости и соответственно могу действитлеьно знаимо быть причинами других переменных. Здесь значения True напротив продукта значат ,что они действительно могут быть причинами других по Йохансену. В результате, совместив эти 2 теста, мы можем получить комбинации переменных, которые являются причинами друг друга."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f405527e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.vector_ar.vecm import coint_johansen\n",
    "\n",
    "def cointegration_test(df, alpha=0.05):\n",
    "    out = coint_johansen(df,-1,5)\n",
    "    d = {'0.90':0, '0.95':1, '0.99':2}\n",
    "    traces = out.lr1\n",
    "    cvts = out.cvt[:, d[str(1-alpha)]]\n",
    "    def adjust(val, length= 6): return str(val).ljust(length)\n",
    "    print('       Тест Йохансена на коинтеграцию  \\n', '--'*20, '\\n', 'Name   ::  Test Stat > C(95%)    =>   Signif  \\n', '--'*20)\n",
    "    for col, trace, cvt in zip(df.columns, traces, cvts):\n",
    "        print(adjust(col), ':: ', adjust(round(trace,2), 9), \">\", adjust(cvt, 8), ' =>  ' , trace > cvt)\n",
    "\n",
    "cointegration_test(df_products) # можно считать достоверными оценки причинности по Гренджеру для тех переменных\n",
    "# для которых оценки теста Йохансена = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fc7876",
   "metadata": {},
   "source": [
    "Здесь мы просто визуализирвоали эти ряды"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c507ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "for products in df_products.columns[19:28]:\n",
    "    fig.add_trace(go.Scatter(x=df_products.index, y=df_products[products], name = products))\n",
    "    fig.update_xaxes(\n",
    "        tickformat=\"%b\\n%Y\")\n",
    "fig.update_xaxes(rangeslider_visible=True, \n",
    "                 title_text = \"Дата\", \n",
    "                 title_standoff = 25, \n",
    "                 rangeselector=dict(\n",
    "                     buttons=list([\n",
    "                         dict(count=1, label=\"1м\", step=\"month\", stepmode=\"backward\"),\n",
    "                         dict(count=6, label=\"6м\", step=\"month\", stepmode=\"backward\"),\n",
    "                         dict(count=1, label=\"1г\", step=\"year\", stepmode=\"backward\"),\n",
    "                         dict(label=\"все\", step=\"all\")])))\n",
    "fig.update_yaxes(title_text = \"Продажи\", title_standoff = 25)\n",
    "fig.update_layout(title = \"Набор причинных временных рядов\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9b2d37",
   "metadata": {},
   "source": [
    "И на последок посмотрим на тепловую карту многомерной автокорреляции переменных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3de03d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def estimate_multivariate_autocorrelation(df):\n",
    "    n_cols = df.shape[1]\n",
    "    autocorrelation_matrix = np.zeros((n_cols, n_cols))\n",
    "    for i in range(n_cols):\n",
    "        for j in range(n_cols):\n",
    "            autocorrelation_matrix[i, j] = df[df.columns[i]].autocorr(lag=j)\n",
    "    return autocorrelation_matrix\n",
    "autocorrelation_matrix = estimate_multivariate_autocorrelation(df_products)\n",
    "def create_autocorrelation_heatmap(autocorrelation_matrix, columns):\n",
    "    data = go.Heatmap(z=autocorrelation_matrix,\n",
    "                      x=columns,\n",
    "                      y=columns,\n",
    "                      hoverongaps = False,\n",
    "                      colorbar=dict(title='Корреляционный коэффициент'))\n",
    "    layout = go.Layout(title='Многомерная автокорреляция')\n",
    "    fig = go.Figure(data=data, layout=layout)\n",
    "    fig.show()\n",
    "\n",
    "create_autocorrelation_heatmap(autocorrelation_matrix, df_products.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ef356b",
   "metadata": {},
   "source": [
    "Таким образом, многие временные ряды коррелируют с другими. Однако, очевидно, что эта корреляция обусловлена сезонностью продаж этих товаров. В ручной настройке мы могли бы разделить уже известные нам группы в словаре временных рядов из раздела \" \", еще на несколько групп, по значениям корреляции между ними: от слабой до сильной. Тем не менее при моделировании благодаря нейронным сетям, мы можем оставить этот момент. И нам следует оставить этот момент, поскольку всё-равно мы не знаем, сколько продукта той или иной категории будет продано в момент n и n+1, а только в момент n-1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84d8a10",
   "metadata": {},
   "source": [
    "## 1.5. Акции\n",
    "<ol>\n",
    "    <li>Что из себя представляет датасет с акциями?</li>\n",
    "    <li>Как часто происходит каждая акция?</li>\n",
    "    <li>Рекламная акция для какого-то продукта влияет на его продажи. Может ли она повлиять на продажи других продуктов?</li>\n",
    "    <li>Есть ли акции, которые пересекаются по времени? Могут ли сразу несколько акций повлиять на один продукт?</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8438784",
   "metadata": {},
   "source": [
    "Информация о датафрейме"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f80819",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pycausalimpact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad45cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import causalimpact\n",
    "df_promotions.info() # информация о наборе данных промоакций"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db99c58",
   "metadata": {},
   "source": [
    "На такой \"частотной\" визуализации в виде ребящего телевизора можно наглядно увидеть, как акции распределены вов ремени. И что они действительно сильно переплетены, а какие-то длятся действительно долго. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1786a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# посмотрим на частоту акций во времени\n",
    "df_promotions.reset_index(inplace=True) \n",
    "df_promo = df_promotions.melt(id_vars=['index'],\n",
    "             var_name = 'name',\n",
    "             value_name = 'promo') # преобразуем широкие данные в длинные\n",
    "\n",
    "fig = px.scatter(df_promo, x=\"index\", y=\"name\", color=\"promo\")\n",
    "\n",
    "fig.update_xaxes(rangeslider_visible=True, \n",
    "                 title_text = \"Дата\", \n",
    "                 title_standoff = 25, \n",
    "                 rangeselector=dict(\n",
    "                     buttons=list([\n",
    "                         dict(count=1, label=\"1м\", step=\"month\", stepmode=\"backward\"),\n",
    "                         dict(count=6, label=\"6м\", step=\"month\", stepmode=\"backward\"),\n",
    "                         dict(count=1, label=\"1г\", step=\"year\", stepmode=\"backward\"),\n",
    "                         dict(label=\"все\", step=\"all\")])))\n",
    "fig.update_layout(\n",
    "    autosize=False,\n",
    "    width=800,\n",
    "    height=800)\n",
    "fig.show() \n",
    "# как можно заметить, есть множество пересечений, частота промоакций различная, они пересекаются\n",
    "# отсюда можно сделать вывод, что акции могут воздействовать на несколько продуктов сразу \n",
    "# кроме того, фундаментально, не зная природу акций, оценить их воздействие на явно сезонные переменные будет затруднительно, а большого эффекта они не окажут"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3205aa",
   "metadata": {},
   "source": [
    "Взаимосвязь переменных промоакций и продуктов продемонстрируем на корреляционной диаграмме."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c594e424",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "def corr(df1, df2):\n",
    "    n = len(df1)\n",
    "    v1, v2 = df1.values, df2.values\n",
    "    sums = np.multiply.outer(v2.sum(0), v1.sum(0))\n",
    "    stds = np.multiply.outer(v2.std(0), v1.std(0))\n",
    "    return pd.DataFrame((v2.T.dot(v1) - sums / n) / stds / n,\n",
    "                        df2.columns, df1.columns)\n",
    "\n",
    "reshapepromo = df_promotions[:-24]\n",
    "reshapepromo = reshapepromo.iloc[:, 1:1001]\n",
    "df_concat_corr = corr(df_products, reshapepromo)\n",
    "au_corr = df_concat_corr.unstack()\n",
    "filtercorr = au_corr[((au_corr >= .6) | (au_corr <= -.6)) & (au_corr !=1.000)]\n",
    "au_corr = filtercorr.unstack(level=0)\n",
    "fig = px.imshow(au_corr, aspect=\"auto\")\n",
    "fig.update_layout(font=dict(size=8))\n",
    "print(\"Топ корреляций переменных\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4b25e2",
   "metadata": {},
   "source": [
    "Тоже самое можно сделать отдельно для всех промоакций."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4b96de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_redundant_pairs(df):\n",
    "    '''Без дублирования'''\n",
    "    pairs_to_drop = set()\n",
    "    cols = df.columns\n",
    "    for i in range(0, df.shape[1]):\n",
    "        for j in range(0, i+1):\n",
    "            pairs_to_drop.add((cols[i], cols[j]))\n",
    "    return pairs_to_drop\n",
    "\n",
    "def get_top_abs_correlations(df, max, min):\n",
    "    '''Получим топ коррелируемых переменных'''\n",
    "    au_corr = df.corr().unstack()\n",
    "    labels_to_drop = get_redundant_pairs(df)\n",
    "    au_corr = au_corr.drop(labels=labels_to_drop).sort_values(ascending=False)\n",
    "    filtercorr = au_corr[((au_corr >= max) & (au_corr <= min)) | ((au_corr <= - max ) & (au_corr >= - min)) & (au_corr !=1.000)]\n",
    "    au_corr = filtercorr.unstack(level=0)\n",
    "    fig = px.imshow(au_corr, aspect=\"auto\")\n",
    "    fig.update_layout(font=dict(size=8))\n",
    "    fig.show()\n",
    "\n",
    "print(\"Топ очень высоких корреляций переменных\")\n",
    "get_top_abs_correlations(reshapepromo, 0.9, 0.999999)\n",
    "print(\"Топ высоких корреляций переменных\")\n",
    "get_top_abs_correlations(reshapepromo, 0.7, 0.899999)\n",
    "print(\"Топ средних корреляций переменных\")\n",
    "get_top_abs_correlations(reshapepromo, 0.5, 0.699999)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c7f6a6",
   "metadata": {},
   "source": [
    "Таким образом, можно понять, что огромное количество промоакций связанно друг с другом и лишь некоторые влияют на продукты.\n",
    "Посмотрим на таблицу промоакций."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a43f69a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_promotions.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b5d8d3",
   "metadata": {},
   "source": [
    "Таблица не очень наглядная, но в ней видно, что из себя представляют данные - 0, когда акций нет, и 1 - когда они есть. Мы можем посчитать, какова длительность каждой акции в промежуток времени её присутствия. Запишем новый датафрейм и визуализируем ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9470c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "df_promo = df_promotions\n",
    "def calculate_promotion_duration(df, promotion_cols):\n",
    "    duration_cols = [col + '_duration' for col in promotion_cols]\n",
    "    df[duration_cols] = 0\n",
    "    for col, duration_col in zip(promotion_cols, duration_cols):\n",
    "        promotion = df[col].to_numpy()\n",
    "        start = -1\n",
    "        for i in range(len(promotion)):\n",
    "            if start == -1:\n",
    "                if promotion[i] == 1:\n",
    "                    start = i\n",
    "            else:\n",
    "                if promotion[i] == 0:\n",
    "                    end = i\n",
    "                    duration = end - start\n",
    "                    df.loc[start:end-1, duration_col] = duration\n",
    "                    start = -1\n",
    "        if start != -1:\n",
    "            end = len(promotion)\n",
    "            duration = end - start\n",
    "            df.loc[start:, duration_col] = duration\n",
    "    return df\n",
    "\n",
    "promotion_cols = list(df_promo.columns)\n",
    "calculate_promotion_duration(df_promo, promotion_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4708be15",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "import plotly.express as px\n",
    "fig = px.line(df_promo.iloc[:, 1002:2002], x=df_promo.index, y=list(df_promo.iloc[:, 1002:2002].columns), labels={'x':'Date','y':'Duration (days)'}, title=\"Promotion duration over time\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc9883a",
   "metadata": {},
   "source": [
    "На таком графике можно даже выделить разные частоты промоакций. Например акции 58 и 978 длятся около 1000 дней. \n",
    "акций, которые длятся более 400 дней меньше, их можно посмотреть на глаз. \n",
    "И большая плотность акций начинается там, где из длительность меньше 400 дней. Здесь уже поможет интерактивность plotly. \n",
    "Тем не менее, разбираться вручную со всем этим разнообразием будет ужасно сложно, долго и непродуктивно. Однако, все новые переменные, которые мы уже собрали, можно \"скормить\" нейросети для лучших предсказаний."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d11f003",
   "metadata": {},
   "source": [
    "## 1.6. Ваш ход\n",
    "Может быть, есть еще что-то интересное, чего мы не заметили? :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5faadd",
   "metadata": {},
   "source": [
    "Уже выложил всё выше :)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cea0d2",
   "metadata": {},
   "source": [
    "***\n",
    "# 2. Прогнозирование\n",
    "В этом блоке предлагается построить прогноз на указанный промежуток времени и ответить на вопросы о метриках и валидации результатов.\n",
    "\n",
    "<i>Возможно, в этом блоке не получится разбить код на предложенные части; в таком случае следует оставить максимально подробный комментарий к происходящему. </i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa21b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "HORIZON = 24 # горизонт прогнозирования"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2175ddf0",
   "metadata": {},
   "source": [
    "## 2.1. Пайплайн прогнозирования\n",
    "### 2.1.1. Подготовка данных\n",
    "<ol>\n",
    "    <li>Нужно ли как-то предобрабатывать ряды из датасета?</li>\n",
    "    <li>Какие признаки можно выделить из данных?</li>\n",
    "    <li>Какие признаки можно извлечь из индекса timestamp?</li>\n",
    "    <li>Как использовать данные об акциях?</li>\n",
    "    <li>*Есть ли среди выделенных признаков categorical признаки? Если есть, как с ними работать?</li>\n",
    "</ol>\n",
    "\n",
    "### 2.1.2. Модель\n",
    "Какие модели прогнозирования могут помочь в нашей задаче? В чем их особенности, плюсы и минусы?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb23e67",
   "metadata": {},
   "source": [
    "За основу возьмём датафрейм промоакций, добавленный их длительностью на предыдущем шаге. Но исключим последние 24 наблюдения для тренировки нейронной сети на известных данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d837da01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import h5py\n",
    "import fix_yahoo_finance as yf\n",
    "pd.core.common.is_list_like = pd.api.types.is_list_like\n",
    "import pandas_datareader.data as pdr\n",
    "from time import sleep\n",
    "from datetime import datetime as dt\n",
    "import talib as tb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "# Libraries required by FeatureSelector()\n",
    "import lightgbm as lgb\n",
    "import gc\n",
    "from itertools import chain\n",
    "from sklearn.cluster import KMeans\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from matplotlib.dates import (DateFormatter, WeekdayLocator, DayLocator, MONDAY)\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, Callback, TensorBoard\n",
    "from keras.models import load_model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Reshape, LSTM, GRU\n",
    "from keras.layers import Conv1D, MaxPooling1D, LeakyReLU, PReLU, GlobalAveragePooling1D\n",
    "from keras import regularizers\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1999dae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Итак, выделим все предикторы в отдельный датафрейм\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "scaler.fit(df_promo.iloc[:, 1002:])\n",
    "duration_norm = pd.DataFrame(scaler.transform(df_promo.iloc[:, 1002:]), columns=df_promo.iloc[:, 1002:].columns).iloc[:,:1000]\n",
    "\n",
    "predictors = pd.DataFrame(df_promo.iloc[:, :1000])\n",
    "predictors[duration_norm.columns] = duration_norm\n",
    "\n",
    "df_X = pd.DataFrame(predictors).iloc[:1071,:]\n",
    "df_X.set_index(\"index\", inplace=True)\n",
    "df_X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3289640",
   "metadata": {},
   "source": [
    "Далее мы добавим в этот датафрейм: \n",
    "\n",
    "\n",
    "* День в году - рассчитанный из индекса. Так нейронная сеть поймёт ежегодную сезонность.\n",
    "\n",
    "\n",
    "Отдельно в функции\n",
    "\n",
    "\n",
    "* Добавим в обучающие данные временной ряд непосредственно продукта, для которого строится нейросеть, сдвинутую на 24 шага вперёд.\n",
    "Таким образом мы сможем затем предсказать данные на 24 шага вперёд, согласно заданию.\n",
    "\n",
    "\n",
    "\n",
    "* Удалим первые 24 пустых наблюдения.\n",
    "\n",
    "\n",
    "\n",
    "И построим модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7ca190",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import train_test_split\n",
    "#train_X, test_X, train_y, test_y = train_test_split(df_X, df_Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a2cf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Y = df_products\n",
    "train_X = df_X\n",
    "train_X['date'] = train_X.index\n",
    "train_X['dayofyear'] = pd.to_datetime(train_X['date']).dt.dayofyear\n",
    "train_X = train_X.drop(columns=['date'])\n",
    "train_y = df_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534b30e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "def build_rnn_model(train_X):\n",
    "    # design network\n",
    "    print(\"\\n\")\n",
    "    print(\"RNN LSTM model architecture >\")\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128, kernel_initializer='random_uniform',\n",
    "                   bias_initializer='zeros', return_sequences=True,\n",
    "                   input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(LSTM(64, kernel_initializer='random_uniform',\n",
    "                   # kernel_regularizer=regularizers.l2(0.001),\n",
    "                   # activity_regularizer=regularizers.l1(0.001),\n",
    "                   bias_initializer='zeros'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(1))\n",
    "    optimizer = keras.optimizers.RMSprop(learning_rate=0.001, rho=0.9, epsilon=1e-08, decay=0.0002)\n",
    "    # optimizer = keras.optimizers.Adagrad(learning_rate=0.03, epsilon=1e-08, decay=0.00002)\n",
    "    # optimizer = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "    # optimizer = keras.optimizers.Nadam(learning_rate=0.0002, beta_1=0.9, beta_2=0.999, schedule_decay=0.004)\n",
    "    # optimizer = keras.optimizers.Adamax(learning_rate=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0)\n",
    "    # optimizer = keras.optimizers.Adadelta(learning_rate=1.0, rho=0.95, epsilon=None, decay=0.0)\n",
    "\n",
    "    model.compile(loss='mae', optimizer=optimizer, metrics=['mse', 'mae'])\n",
    "    model.summary()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731d153d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models_for_each_column(train_X, train_y, model, model_type):\n",
    "    # Get the column names of the dataframe\n",
    "    col_names = train_y.columns\n",
    "    \n",
    "    # Create a dictionary to store the models for each column\n",
    "    models = {}\n",
    "    \n",
    "    for col in col_names:\n",
    "        # Extract the train_X and train_y for the current column\n",
    "        train_y_col = train_y[col]\n",
    "        scaler = MinMaxScaler()\n",
    "        \n",
    "        train_X_col = train_X\n",
    "        train_X_col[col] = train_y[col].shift(24, axis = 0)\n",
    "        \n",
    "        train_X_col = train_X.iloc[24:]\n",
    "        \n",
    "        train_X_col = scaler.fit_transform(train_X_col.values)\n",
    "        train_X_col = train_X_col.reshape((train_X_col.shape[0], 1, train_X_col.shape[1]))\n",
    "        train_y_col = train_y_col.iloc[24:]\n",
    "        train_y_col = scaler.fit_transform(train_y_col.values.reshape(-1,1))\n",
    "        model = build_rnn_model(train_X_col)\n",
    "        if model_type == \"LSTM\":\n",
    "            batch_size=10\n",
    "            mc = ModelCheckpoint('best_lstm_model_{}.h5'.format(col), monitor='val_loss', save_weights_only=False,\n",
    "                             mode='min', verbose=1, save_best_only=True)\n",
    "        elif model_type == \"CNN\":\n",
    "            batch_size=8\n",
    "            mc = ModelCheckpoint('best_cnn_model_{}.h5'.format(col), monitor='val_loss', save_weights_only=False,\n",
    "                             mode='min', verbose=1, save_best_only=True)\n",
    "        # fit network\n",
    "        history = model.fit(\n",
    "            train_X_col, train_y_col, epochs=50, batch_size=batch_size, validation_split=0.2, \n",
    "            shuffle=False, callbacks=[mc])\n",
    "        models[col] = model\n",
    "    \n",
    "    return models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b234199a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "models = train_models_for_each_column(train_X, train_y, build_rnn_model, \"LSTM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8d7211",
   "metadata": {},
   "source": [
    "Теперь просто выполним те же шаги для целевых данных, чтобы предсказать движение продаж на 24 шага вперёд."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5272bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "scaler.fit(df_promo.iloc[:, 1002:])\n",
    "duration_norm = pd.DataFrame(scaler.transform(df_promo.iloc[:, 1002:]), columns=df_promo.iloc[:, 1002:].columns).iloc[:,:1000]\n",
    "\n",
    "predictors = pd.DataFrame(df_promo.iloc[:, :1000])\n",
    "predictors[duration_norm.columns] = duration_norm\n",
    "\n",
    "df_X = pd.DataFrame(predictors)\n",
    "df_X.set_index(\"index\", inplace=True)\n",
    "df_X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb58057",
   "metadata": {},
   "source": [
    "**Сравнение оригинального ряда и наших предсказаний приведено ниже на графиках.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6515ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Y = df_products\n",
    "train_X = df_X\n",
    "train_X['date'] = train_X.index\n",
    "train_X['dayofyear'] = pd.to_datetime(train_X['date']).dt.dayofyear\n",
    "train_X = train_X.drop(columns=['date'])\n",
    "train_y = df_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bea159b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "\n",
    "# Create a subplot for each column\n",
    "\n",
    "\n",
    "# Iterate through the columns of the original dataframe\n",
    "for i, col in enumerate(train_y.columns, 1):\n",
    "    df = pd.DataFrame()\n",
    "    df['Date'] = df_X.index\n",
    "    # Get the original values and predictions for the current column\n",
    "    original = train_y[col]\n",
    "    original = scaler.fit_transform(original.values.reshape(-1,1))\n",
    "    train_y_col = train_y[col]\n",
    "        \n",
    "    train_X_col = train_X\n",
    "    train_X_col[col] = train_y[col]\n",
    "    train_X_col[col] = train_X_col[col].shift(24, axis = 0)\n",
    "        \n",
    "    train_X_col = train_X.iloc[24:]\n",
    "        \n",
    "    train_X_col = scaler.fit_transform(train_X_col.values)\n",
    "    train_X_col = train_X_col.reshape((train_X_col.shape[0], 1, train_X_col.shape[1]))\n",
    "    train_y_col = train_y_col.iloc[24:]\n",
    "    train_y_col = scaler.fit_transform(train_y_col.values.reshape(-1,1))\n",
    "    \n",
    "    predictions = models[col].predict(train_X_col)\n",
    "    \n",
    "    df['original'] = pd.DataFrame(original)\n",
    "    df['predictions'] = pd.DataFrame(predictions)\n",
    "    df['predictions'] = df['predictions'].shift(24, axis = 0)\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    # Create a trace for the original values\n",
    "    original_trace = go.Scatter(x=df['Date'], y=df['original'], name='Original', mode='lines', line=dict(color='purple'))\n",
    "    \n",
    "    # Create a trace for the predictions\n",
    "    predictions_trace = go.Scatter(x=df['Date'], y=df['predictions'], name='Predictions', mode='lines', line=dict(color='green'))\n",
    "    \n",
    "    # Add the traces to the subplot\n",
    "    fig.add_trace(predictions_trace)\n",
    "    fig.add_trace(original_trace)\n",
    "    fig.update_xaxes(rangeslider_visible=True, \n",
    "                 title_text = \"Дата\", \n",
    "                 title_standoff = 25, \n",
    "                 rangeselector=dict(\n",
    "                     buttons=list([\n",
    "                         dict(count=1, label=\"1м\", step=\"month\", stepmode=\"backward\"),\n",
    "                         dict(count=6, label=\"6м\", step=\"month\", stepmode=\"backward\"),\n",
    "                         dict(count=1, label=\"1г\", step=\"year\", stepmode=\"backward\"),\n",
    "                         dict(label=\"все\", step=\"all\")])))\n",
    "    \n",
    "    fig.update_layout(title=col + ' Orig vs Pred')\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edbaf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_deviation = recovered_data_lstm.loc[test_set.index][['Predict-Y']] - original_stock_context_fs_full.loc[test_set.index][['Predict-Y']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb26a9b",
   "metadata": {},
   "source": [
    "## 2.2. Валидация \n",
    "### 2.2.1. Метрики\n",
    "<ol>\n",
    "    <li>Какие метрики качества могут быть использованы в нашей задаче?</li>\n",
    "    <li>В качестве метрики качества мы хотим использовать MSE; с какими проблемами мы можем столкнуться?</li>\n",
    "</ol>\n",
    "\n",
    "### 2.2.2. Кросс-валидация\n",
    "Как провести кросс-валидацию?\n",
    "\n",
    "### 2.2.3. Сравнение моделей\n",
    "Предположим, мы построили несколько пайплайнов прогнозирования. Как выбрать лучший из них?\n",
    "<ol>\n",
    "    <li>В датасете 30 рядов, мы посчитали метрику для каждого из них, но нам надо понять, какой из пайплайнов работает лучше; как это сделать?</li>\n",
    "    <li>Мы выбрали лучший из пайплайнов; можно ли еще улучшить его? Когда стоит остановиться?</li>\n",
    "    <li>*Если в предыдущих частях были рассмотрены несколько пайплайнов, какой оказался лучшим? Как выглядит лучший прогноз? (Если выше был рассмотрен один пайплайн, пропустите этот пункт)</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb5f055",
   "metadata": {},
   "source": [
    "**К сожалению, ответы на все эти вопросы зашиты в нейросеть.\n",
    "За исключением кросс-валидации.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a4e862",
   "metadata": {},
   "source": [
    "## 2.3. Использование доп данных\n",
    "<ol>\n",
    "    <li>Получилось ли использовать данные об акциях при построении прогнозов?</li>\n",
    "    <li>Если да, помогают ли они предсказывать точнее?</li>\n",
    "    <li>Как понять, какие из акций важны, а какие нет? Могут ли скоррелированные признаки помешать оценке важности признака? Что с этим делать?</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53667246",
   "metadata": {},
   "source": [
    "**Ответы:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5eb970",
   "metadata": {},
   "source": [
    "1. Да, данные об акциях были использованы и даже дополнены. Однако, на моё взгляд, столько данных даже избыточно. Возможно, стоило бы взять для оценки только те промоакции, которые коррелируют с временными рядами. Это момент для улучшения. \n",
    "2. Они помогают определять всплески активности.\n",
    "3. Коллинераность прмооакций можно устранить, удаляя коллинеарные переменные из данных. Для этого можно воспользвоаться VIF-тестом."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8678f8",
   "metadata": {},
   "source": [
    "## 2.4. Production 🚀\n",
    "Мы построили восхитительный пайплайн прогнозирования! Как вывести его в продакшн?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c9941f",
   "metadata": {},
   "source": [
    "Необходимые шаги для простого продакшна будут выглядет следующим образом:\n",
    "\n",
    "Упаковать пайплайн: setuptools, pip или conda, чтобы упаковать пайплайн в виде библиотеки. Это упростит установку и распространение пайплайна на другие машины.\n",
    "\n",
    "Docker или другой инструмент контейнеризации, чтобы упаковать пайплайн и его зависимости в контейнер. Это упростит развертывание пайплайна на любом компьютере, поддерживающем среду выполнения контейнера.\n",
    "\n",
    "Выберем метод развертывания, который лучше всего подходит для использования, например, с использованием облачной платформы, такой как AWS, GCP или Azure, или развертывание на локальном компьютере.\n",
    "\n",
    "СМожно создать API: Flask или Django, чтобы создать API, который позволит пользователям взаимодействовать с пайплайном.\n",
    "\n",
    "Настройка мониторинга: такие инструменты, как prometheus или стек ELK, для мониторинга и регистрации пайплайна, чтобы обеспечить его бесперебойную работу.\n",
    "\n",
    "Автоматизация тестирования: unittest или pytest, чтобы написать автоматические тесты для пайплайна, чтобы убедиться, что он работает должным образом, прежде чем развертывать его в рабочей среде.\n",
    "\n",
    "Оптимизация производительности.\n",
    "\n",
    "Контроль версий: git, чтобы отслеживать изменения в пайплайне и при необходимости откатываться до предыдущей версии.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f93823-1378-4db1-bd76-1a1ebda68054",
   "metadata": {
    "tags": []
   },
   "source": [
    "***\n",
    "# Свободная часть\n",
    "Часть для самых смелых энтузиастов в мире временных рядов! \n",
    "\n",
    "Здесь предлагается попробовать сделать с датасетом что-то интересное на ваш вкус. Можно попробовать сделать что-то из предложенного:\n",
    "- Покрутить датасет `dataset/influence.csv`: в нем дана матрица влияния каждой акции на каждый временной ряд продаж. Как ее можно использовать? Как это может помочь при прогнозировании? \n",
    "- Поглубже погрузиться в изучение \"близости\" временнях рядов и попробовать использовать эти знания для прогнозирования\n",
    "- Подумать о том, как можно оценить влияние какого-то внешнего фактора-признака? Допустим, у нас есть такой же датасет с продажами и мы знаем, что определенная акция должна была повлиять на конкрентный ряд. Как оценить это влияние?\n",
    "- Любая другая тема, которая кажется важной и интересной"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946f7737",
   "metadata": {},
   "source": [
    "В другой раз)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
